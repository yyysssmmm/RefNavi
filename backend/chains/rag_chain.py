# ğŸ“ íŒŒì¼: backend/chains/rag_chain.py

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from langchain.schema import Generation, LLMResult
from dotenv import load_dotenv
from typing import List
import os

load_dotenv()

# âœ… 1. ë²¡í„° DB ë° ì„ë² ë”© ë¡œë“œ
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = Chroma(
    collection_name="refnavi_abstracts",
    embedding_function=embedding
)

# âœ… 2. Retriever êµ¬ì„±
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# âœ… 3. í…ŒìŠ¤íŠ¸ìš© Mock LLM ì •ì˜
class MockLLM(LLM):

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        return "This is a mock response for testing."

    def _generate(self, prompts, **kwargs):
        return LLMResult(generations=[[Generation(text="This is a mock response for testing.")]])

    @property
    def _llm_type(self):
        return "mock-llm"

llm = MockLLM()

# âœ… 4. RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# âœ… 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    query = "What is the contribution of the Transformer paper?"
    result = qa_chain.invoke(query)

    print("\nğŸ“¢ ë‹µë³€:\n", result['result'])
    print("\nğŸ“š ì°¸ê³ í•œ ë¬¸ì„œë“¤:")
    for doc in result['source_documents']:
        print("-", doc.metadata.get("title"))
