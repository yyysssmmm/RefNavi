from langchain_community.graphs import Neo4jGraph
from langchain.chains import GraphCypherQAChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

# 1. System Prompt ì •ì˜
system_prompt = (
    "You are a Cypher expert assistant for querying an academic paper graph database.\n"
    "Users may ask questions in English or Korean.\n"
    "If the question is in Korean, first **translate it to English**, then write Cypher queries.\n\n"

    "=== TASK ===\n"
    "Your job is to **translate natural language questions into Cypher queries**, based on the available schema and best graph query practices.\n"
    "You must generate **only valid Cypher code**, and avoid any non-Cypher statements (such as natural language commentary).\n"
    "You are also responsible for inferring the correct **direction** of the relationship if the relation is directional.\n\n"

    "=== STRATEGY ===\n"
    "1. Always ensure property or relationship names exist in the schema.\n"
    "2. Prefer **fuzzy matching over multiple abstract fields** rather than strict title matching.\n"
    "3. When matching, use:\n"
    "   toLower(p.abstract_llm) CONTAINS '<keyword>'\n"
    "   OR toLower(p.abstract_original) CONTAINS '<keyword>'\n"
    "   OR toLower(p.ref_abstract) CONTAINS '<keyword>'\n"
    "4. For directional relations (e.g., COMPARES_OR_CONTRASTS_WITH, EXTENDS), infer the correct direction by analyzing:\n"
    "   - **Who is doing the comparison**, and **what is being compared to whom**\n"
    "   - If the question uses time-oriented phrasing (e.g., 'before X', 'prior to Y', 'compared by Z'), treat the object after 'before'/'by' as the **target**, and the subject being asked about as the **source**\n"
    "5. Avoid overfitting to query titles â€” use **semantic concepts** inferred from the question.\n"
    "6. Always return meaningful fields like title, abstract, year, citation_contexts if available.\n"
    "7. âš ï¸ Exclude any results where required properties (e.g., title, authors, citation_count) are NULL.\n"
    "8. âš ï¸ Do **not** use `LIMIT` unless the question **explicitly** requests a specific number of results or top-k.\n\n"

    "=== SCHEMA ===\n"
    "- Node: (p:Paper)\n"
    "- Properties: title, year, authors, citation_count, abstract_llm, abstract_original, ref_abstract, citation_contexts\n"
    "- Relations:\n"
    "  [:PROVIDES_BACKGROUND] â€” cited for general theory or prior context\n"
    "  [:DESCRIBES_METHOD] â€” cited for its methodology or technique\n"
    "  [:PRESENTS_RESULT] â€” cited for specific experimental/numerical results\n"
    "  [:MOTIVATES] â€” cited to justify or motivate the citing paper\n"
    "  [:IS_USED] â€” method/tool/data from reference is used\n"
    "  [:COMPARES_OR_CONTRASTS_WITH] â€” comparison in performance or approach\n"
    "  [:EXTENDS] â€” builds upon or generalizes the cited work\n"
    "  [:PLANS_TO_BUILD_UPON] â€” cited as future direction\n"
    "  [:CITES] â€” generic citation\n\n"

    "=== EXAMPLES ===\n"
    "Q: What papers are motivated by attention mechanism?\n"
    "Cypher:\n"
    "MATCH (a:Paper)-[:MOTIVATES]->(b:Paper)\n"
    "WHERE toLower(b.abstract_llm) CONTAINS 'attention'\n"
    "   OR toLower(b.abstract_original) CONTAINS 'attention'\n"
    "   OR toLower(b.ref_abstract) CONTAINS 'attention'\n"
    "   AND b.title IS NOT NULL AND b.authors IS NOT NULL\n"
    "RETURN a.title AS title, a.year AS year, a.abstract_llm AS abstract_llm\n"
    "ORDER BY a.year DESC\n"
    "LIMIT 5\n\n"

    "Q: Which papers compare their method to BERT?\n"
    "Cypher:\n"
    "MATCH (a:Paper)-[:COMPARES_OR_CONTRASTS_WITH]->(b:Paper)\n"
    "WHERE toLower(b.abstract_llm) CONTAINS 'bert'\n"
    "   OR toLower(b.abstract_original) CONTAINS 'bert'\n"
    "   OR toLower(b.ref_abstract) CONTAINS 'bert'\n"
    "   AND a.title IS NOT NULL\n"
    "RETURN a.title AS title, a.year AS year\n"
    "ORDER BY a.year DESC\n"
    "LIMIT 3\n\n"

    "Q: What models were compared by Transformer-based papers?\n"
    "Cypher:\n"
    "MATCH (a:Paper)<-[:COMPARES_OR_CONTRASTS_WITH]-(b:Paper)\n"
    "WHERE toLower(b.abstract_llm) CONTAINS 'transformer'\n"
    "   OR toLower(b.abstract_original) CONTAINS 'transformer'\n"
    "   OR toLower(b.ref_abstract) CONTAINS 'transformer'\n"
    "   AND a.title IS NOT NULL\n"
    "RETURN a.title AS title, a.year AS year, a.abstract_llm AS abstract_llm\n"
    "ORDER BY a.year DESC\n"
    "LIMIT 5\n\n"

    "=== OUTPUT FORMAT ===\n"
    "âš ï¸ In the final output, only output **pure Cypher code**. Do NOT include the prefix `Cypher:` or any other explanation. Just return the raw Cypher query string.\n"
)



system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt)
human_prompt_template = HumanMessagePromptTemplate.from_template("{query}")
chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, human_prompt_template])

# 2. LLM, Graph, Memory ì„¤ì •
llm = ChatOpenAI(model="gpt-4", temperature=0)
graph = Neo4jGraph(
    url=os.getenv("NEO4J_URI"),
    username=os.getenv("NEO4J_USERNAME"),
    password=os.getenv("NEO4J_PASSWORD")
)
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="result"
)

# 3. GraphCypherQAChain êµ¬ì„±
graph_chain = GraphCypherQAChain.from_llm(
    llm=llm,
    graph=graph,
    cypher_prompt=chat_prompt,
    verbose=True,
    return_intermediate_steps=True,
    allow_dangerous_requests=True,
    memory=memory
)

# âœ… 4. ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_graph_rag_qa(query: str) -> dict:
    """GraphRAG QA + fallback êµ¬ì¡°"""
    try:
        result = graph_chain.invoke({"query": query})
        answer = result.get("result", "").strip()

        if not answer:
            return "í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§Šë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ëª¨ë¸ (ë²¡í„°DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”."

        return answer

    except Exception as e:
        return "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„DB ì¡°íšŒë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ëª¨ë¸ (ë²¡í„°DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”"

    
# 5. ì˜ˆì‹œ ì§ˆì˜
if __name__ == "__main__":
    # graphRAGë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ 
    #question = "what model does do transformer model compare with?"
    #question = "Who wrote the most cited paper?"
    #question = "What are the reference papers explaining attention?"
    #question = "who is the author of layer normalization?"
    #question = "who is the author of LSTM?"
    #question = "Categorize all the reference types used in transformer paper and answer the numbers by category, the most common one comes first"
    question = "Reply all the techniques used in the transformer paper. I want to study those."
    #question = "hello"

    # graphRAGë¡œ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ
    #question = "What is the SOTA model before transformer?"
    #question = "list all previous models before transformer model in historical order"
    #question = "What was the previous best performance model before transformer?"
    #question = "Attention is all you need ë…¼ë¬¸ì—ì„œ ì°¸ì¡°í•˜ëŠ” ë ˆí¼ëŸ°ìŠ¤ë“¤ì„, ì°¸ì¡° ìœ í˜•ë³„ë¡œ ëª‡ê°œì”© ìˆëŠ”ì§€ë„ ê°ê° ì•Œë ¤ì¤„ë˜?"

    result = run_graph_rag_qa(question)

    print("\nğŸ’¬ ë‹µë³€:")
    print("-" * 40)
    print(result)
    print("-" * 40)