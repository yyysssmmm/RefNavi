from langchain_community.graphs import Neo4jGraph
from langchain.chains import GraphCypherQAChain
from langchain_core.prompts.chat import (
    ChatPromptTemplate, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate
)
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

# 1. System Prompt ì •ì˜
system_prompt = (
    """You are a Cypher expert assistant for querying an academic paper graph database.

=== TASK ===
Your job is to **translate natural language questions into Cypher queries**, based on the available schema and best practices.
You must generate **only valid Cypher code** and avoid any non-Cypher statements (such as natural language commentary).
You are also responsible for inferring the correct **directionality** of relationships when applicable.

You must **NOT include the user's question, translation, or any explanation** in the output.  
Just output the Cypher query directly.

=== STRATEGY ===
1. Always ensure that property or relationship names exist in the schema.

2. ğŸ§­ First determine which paper is the **citing paper** and which is the **cited paper**.
   - The **citing paper** is the one doing the referencing â€” typically the uploaded paper or the main subject of the user's question.
   - The **cited paper** is the one being referenced â€” typically the reference in question.

   Use chat history and the phrasing of the current question to decide:
   - If the user previously uploaded a paper, treat it as the **citing paper**.
   - If the question refers to "this paper", "the uploaded paper", or no specific title, assume it refers to the **citing paper**.
   - If the question refers to a named paper (e.g., "What does Transformer compare itself to?"), that paper is likely the **citing paper**.

3. Once citing and cited roles are determined, apply **case-insensitive fuzzy matching** to the appropriate fields:

   For the **citing paper**, match over:
   - `toLower(p.abstract_llm)`
   - `toLower(p.abstract_original)`
   - `toLower(p.title)`

   For the **cited paper**, match over:
   - `toLower(p.ref_abstract)`
   - `toLower(p.title)`

   Use `OR` to combine all fields into a single robust match condition.  
   â—Avoid using exact title match unless explicitly instructed.

4. For directional relationships (e.g., COMPARES_OR_CONTRASTS_WITH, HAS_BACKGROUND_ON, EXTENDS_IDEA_OF), infer direction based on:
   - Who is making the claim or comparison
   - Time-based phrasing (e.g., "before X" â†’ X is the **cited**, the other is the **citing**)

5. Always return meaningful fields: `title`, `abstract`, `year`, and `citation_contexts` if available.

6. âš ï¸ Exclude any results where essential properties (e.g., title, authors, citation_count) are NULL.

7. âš ï¸ Do **not** use `LIMIT` unless the user explicitly requests a specific number of results or says "top-k".

8. Do not include any natural language instructions, questions, or summaries in your output â€” return only the valid Cypher query.


=== SCHEMA ===
- Node: (p:Paper)
- Relationships:
  All citation relationships follow this directional structure (be aware of edge direction):
  **(a:Paper)-[:RELATION]->(b:Paper)**
  where:
    - `a` is the **citing paper** (uploaded paper)
    - `b` is the **cited paper** (reference paper)

  Available relationships:
  (a)-[:HAS_BACKGROUND_ON]->(b): b provides background for a.
  (a)-[:USE_METHOD_OF]->(b): a uses or adapts a method, dataset, or technique from b.
  (a)-[:IS_MOTIVATED_BY]->(b): a is motivated or inspired by b.
  (a)-[:COMPARES_OR_CONTRASTS_WITH]->(b): a compares or contrasts itself with b.
  (a)-[:EXTENDS_IDEA_OF]->(b): a extends, generalizes, or builds upon an idea from b.

  Properties:
    - Citing paper (`a`): `title`, `abstract_llm`, `abstract_original`
    - Cited paper (`b`): `title`, `year`, `authors`, `citation_count`, `ref_abstract`, `citation_contexts`

=== EXAMPLES ===
â—ï¸Avoid using `title` for keyword matching. Use abstract-based fuzzy matching instead.

âŒ Bad example:
MATCH (p:Paper)
WHERE toLower(p.title) ='transformer'
RETURN p.title

âœ… Good example:
MATCH (a:Paper)-[]->(b:Paper)
WHERE toLower(a.abstract_llm) CONTAINS 'transformer'
OR toLower(a.abstract_original) CONTAINS 'transformer'
OR toLower(b.ref_abstract) CONTAINS 'transformer'
RETURN b.title, b.year, b.citation_count

=== OUTPUT FORMAT ===
âš ï¸ Your output MUST be **only valid Cypher code** â€” no explanation, no natural language, no comments, and no user question included.

âŒ Examples of forbidden output:
- â€œTranslate: ...â€ â†’ âŒ
- â€œHere is the query:â€ â†’ âŒ
- `Cypher:` â†’ âŒ

âœ… Just output **pure Cypher code**, starting directly from:
MATCH ...
RETURN ...

ğŸ›‘ Even a single extra line (e.g., user's question or explanation) will invalidate the output.
"""
)

llm = ChatOpenAI(model="gpt-4", temperature=0)

graph = Neo4jGraph(
    url=os.getenv("NEO4J_URI"),
    username=os.getenv("NEO4J_USERNAME"),
    password=os.getenv("NEO4J_PASSWORD")
)


# âœ… 4. ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_graph_rag_qa(query: str, chat_history: list = []) -> str:
    """
    chat_historyë¥¼ ë°˜ì˜í•œ Cypher í”„ë¡¬í”„íŠ¸ ìƒì„± + Graph QA ì‹¤í–‰
    """

    try:
        # 1. system message
        system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt)

        # 2. íˆìŠ¤í† ë¦¬ ë°˜ì˜ ë° ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        human_prompt_template = HumanMessagePromptTemplate.from_template("{query}")
        chat_prompt = ChatPromptTemplate.from_messages(
            [system_prompt_template] + chat_history + [human_prompt_template]
        )

        # 3. chain ìƒì„± ë° ì‹¤í–‰
        graph_chain = GraphCypherQAChain.from_llm(
            llm=llm,
            graph=graph,
            cypher_prompt=chat_prompt,
            verbose=True,
            return_intermediate_steps=True,
            allow_dangerous_requests=True
        )

        result = graph_chain.invoke({"query": query})

        # 4. Cypher ê²°ê³¼ í™•ì¸
        intermediate = result.get("intermediate_steps", [])
        context_docs = intermediate[1].get("context", []) if len(intermediate) > 1 else []

        print("âœ… context_docs:", context_docs)

        if not context_docs:
            return "í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§ˆë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”."

        return result.get("result", "").strip()

    except Exception as e:
        print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
        return "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„ DB ì¡°íšŒë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”."

    
# 5. ì˜ˆì‹œ ì§ˆì˜
if __name__ == "__main__":
    # graphRAGë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ 
    #question = "Attention is all you need ë…¼ë¬¸ì—ì„œ ì°¸ì¡°í•˜ëŠ” ë ˆí¼ëŸ°ìŠ¤ë“¤ì„, ì°¸ì¡° ìœ í˜•ë³„ë¡œ ëª‡ê°œì”© ìˆëŠ”ì§€ë„ ê°ê° ì•Œë ¤ì¤„ë˜?"
    
    #question = "what model does do transformer model compare with?"
    #question = "Who wrote the most cited paper?"
    #question = "What are the reference papers explaining attention?"
    #question = "who is the author of layer normalization?"
    #question = "who is the author of LSTM?"
    #question = "Categorize all the reference types used in transformer paper and answer the numbers by category, the most common one comes first"
    #question = "Reply all the techniques used in the transformer paper. I want to study those."
    #question = "What is the SOTA model before transformer?"
    #question = "hello"
    question = "transformer ë…¼ë¬¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"

    # graphRAGë¡œ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ
    #question = "Attention is all you needì˜ ë°°ê²½ì´ ë˜ì—ˆë˜ ëª¨ë¸ì— ëŒ€í•´ì„œ ê³¼ê±° ìˆœìœ¼ë¡œ ì•Œë ¤ì¤˜"
    
    question = "list all previous models before transformer model in historical order"
    #question = "What was the previous best performance model before transformer?"


    result = run_graph_rag_qa(question, [])

    print("\nğŸ’¬ ë‹µë³€:")
    print("-" * 40)
    print(result)
    print("-" * 40)