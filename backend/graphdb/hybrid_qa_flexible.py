# hybrid_qa.py

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from vectorstore.vector_qa import run_qa_chain
from graphdb.graph_qa import run_graph_rag_qa  # âœ… fallback ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©


base_dir = os.path.join(os.path.dirname(__file__), "../utils/metadata")
VECTOR_DB_DIR = os.path.join(base_dir, "chroma_db")

llm = ChatOpenAI(model="gpt-4", temperature=0)


# âœ… ë²¡í„° ë¬¸ì„œ title ìš”ì•½ìš© í•¨ìˆ˜
def format_vector_titles(docs: list[Document]) -> str:
    if not docs:
        return "[Vector DB] No documents retrieved."

    result = "[Vector DB] The following document titles were retrieved by similarity search:\n\n"
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get("title", "Unknown Title")
        result += f"{i}. {title}\n"
    return result.strip()


# âœ… Hybrid QA ì‹¤í–‰ í•¨ìˆ˜
def hybrid_qa(
    question: str,
    vector_db_dir=VECTOR_DB_DIR,
    k: int = 3,
    return_sources=False,
    chat_history=[]
):
    print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")

    # âœ… 1. Graph QA ì‹¤í–‰ (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
    graph_answer = run_graph_rag_qa(question, chat_history)

    # âœ… 2. Vector QA ì‹¤í–‰
    vector_answer, sources = run_qa_chain(
        question, k=k, VECTOR_DB_DIR=vector_db_dir, return_sources=True, chat_history=chat_history
    )
    vector_docs_summary = format_vector_titles(sources)

    # âœ… 3. System Prompt ì •ì˜
    system_template = SystemMessagePromptTemplate.from_template(
        """You are a helpful assistant. The user may ask in any language, and you must respond in that same language.

    You are provided with three sources of information to answer the user's question:

    - ğŸ“˜ **Vector DB Answer**: {vector_answer}  
    Document titles retrieved from Vector DB: {vector_docs_summary}

    - ğŸ”— **Graph DB Answer**: {graph_answer}

    - ğŸ§  **Your own general knowledge**

    ğŸ¯ Your task is to synthesize a comprehensive, accurate, and helpful response by **integrating all three sources**:
    1. Use specific facts and insights from **Vector DB and Graph DB** whenever possible â€” such as citation contexts, abstract content, relationships, or keywords.
    2. Feel free to supplement with **your own general knowledge or reasoning** to fill in any missing details, clarify vague parts, or connect ideas.
    3. You must meaningfully incorporate **all three sources**, but you may emphasize the one that contributed most to answering the user's question.
    - Reflect this emphasis **naturally in the tone or content** of your answer, **not by directly naming the source**.

    ğŸ’¡ Think of yourself as a research assistant combining multiple views to give the best possible explanation â€” clear, informative, and well-grounded.

    âš ï¸ Never mention the sources explicitly in your answer (e.g., do not say "According to the Graph DB...").
    âœ… Instead, at the end of your answer, include the following tag indicating which sources were most helpful:
    [Answer Source: (e.g., Mainly Graph DB + Own Knowledge)]
    """
    )


    # âœ… 4. íˆìŠ¤í† ë¦¬ ë°˜ì˜

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_template] + chat_history + [
            HumanMessagePromptTemplate.from_template("{question}")
        ]
    )

    # âœ… 5. LLM ì‹¤í–‰ ì²´ì¸
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({
        "question": question,
        "vector_answer": vector_answer,
        "vector_docs_summary": vector_docs_summary,
        "graph_answer": graph_answer
    })

    # âœ… 6. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if chat_history is not None:
        chat_history.append(HumanMessage(content=question))
        chat_history.append(AIMessage(content=response))

    print("\nğŸ“Œ Hybrid QA Result:")
    print(response)

    return (response, sources) if return_sources else (response, [])


if __name__ == "__main__":
    question = "ì•ˆë…•"

    response, _ = hybrid_qa(
        question=question,
        k=3,
        return_sources=False
        )

    print(response)