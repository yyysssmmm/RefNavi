# hybrid_qa.py

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from vectorstore.vector_qa import run_qa_chain
from graphdb.graph_qa import run_graph_rag_qa  # âœ… fallback ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©


base_dir = os.path.join(os.path.dirname(__file__), "../utils/metadata")
VECTOR_DB_DIR = os.path.join(base_dir, "chroma_db")

llm = ChatOpenAI(model="gpt-4", temperature=0)


# âœ… ë²¡í„° ë¬¸ì„œ title ìš”ì•½ìš© í•¨ìˆ˜
def format_vector_titles(docs: list[Document]) -> str:
    if not docs:
        return "[Vector DB] No documents retrieved."

    result = "[Vector DB] The following document titles were retrieved by similarity search:\n\n"
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get("title", "Unknown Title")
        result += f"{i}. {title}\n"
    return result.strip()


# âœ… Hybrid QA ì‹¤í–‰ í•¨ìˆ˜
def hybrid_qa(
    question: str,
    vector_db_dir=VECTOR_DB_DIR,
    k: int = 3,
    return_sources=False,
    chat_history=[]
):
    print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")

    # âœ… 1. Graph QA ì‹¤í–‰ (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
    graph_answer = run_graph_rag_qa(question, chat_history=chat_history)

    graph_failure_msgs = ["í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§ˆë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”.",
                    "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„ DB ì¡°íšŒë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”."]

    # âœ… 2. Graph ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•  ê²½ìš° Vector QA ì‹¤í–‰
    if graph_answer in graph_failure_msgs:
        vector_answer, sources = run_qa_chain(
            question, k=k, VECTOR_DB_DIR=vector_db_dir, return_sources=True, chat_history=chat_history
        )
        vector_docs_summary = format_vector_titles(sources)
    else:
        vector_answer, sources = "ê·¸ë˜í”„ DBì—ì„œ ì´ë¯¸ ì¶©ë¶„í•œ ë‚´ìš©ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„°DBë¥¼ ê²€ìƒ‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", []
        vector_docs_summary = ""

    # âœ… 3. System Prompt ì •ì˜
    system_template = SystemMessagePromptTemplate.from_template(
        """You are a helpful assistant. The user may ask questions in any language, and you must respond in the same language.

    You are given two optional answers to assist with your response:

    - Answer A (from Graph DB): {graph_answer}
    - Answer B (from Vector DB): {vector_answer}

    You are also given a list of related document titles retrieved from the Vector DB to help assess the relevance of its content:
    {vector_docs_summary}

    ---

    ğŸ¯ Your task: Choose the **most relevant, informative, and complete** answer source to respond to the userâ€™s question.

    ğŸ” Source selection rules:

    1. **Evaluate the Graph DB answer first**:
        - Use it **only if** it contains specific factual information that clearly and directly addresses the userâ€™s question.
        Examples include: citation counts, author names, or explicit citation relationships.
        - **Do not use it** if it contains generic fallback text such as:
            - "í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§ˆë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            - "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤."

    2. **Then evaluate the Vector DB answer and its context**:
        - Use it **only if**:
            - The vector answer is informative and helpful,
            - AND the related document titles are clearly relevant to the user's question.
        - **Do not use it** if it contains fallback phrases such as:
            - "í˜„ì¬ êµ¬ì¶•ëœ ë²¡í„° DBì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            - "ê·¸ë˜í”„ DBì—ì„œ ì´ë¯¸ ì¶©ë¶„í•œ ë‚´ìš©ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„°DBë¥¼ ê²€ìƒ‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    3. **Fallback to your own knowledge**:
        - If **neither** source is informative or relevant, answer using your own general academic knowledge and reasoning.

    ---

    âš ï¸ Important Instructions:
    - Never choose a source **just because it exists**.
    - Base your choice on **actual usefulness and content quality**, not mere presence.
    - Be especially cautious with known fallback or template-like responses.

    âœ… At the end of your answer, append exactly one of the following:
    - [Answer Source: Graph DB]
    - [Answer Source: Vector DB]
    - [Answer Source: Own Knowledge]
    """
    )


    # âœ… 4. íˆìŠ¤í† ë¦¬ ë°˜ì˜

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_template] + chat_history + [
            HumanMessagePromptTemplate.from_template("{question}")
        ]
    )

    # âœ… 5. LLM ì‹¤í–‰ ì²´ì¸
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({
        "question": question,
        "vector_answer": vector_answer,
        "vector_docs_summary": vector_docs_summary,
        "graph_answer": graph_answer
    })

    # âœ… 6. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if chat_history is not None:
        chat_history.append(HumanMessage(content=question))
        chat_history.append(AIMessage(content=response))

    print("\nğŸ“Œ Hybrid QA Result:")
    print(response)

    return (response, sources) if return_sources else (response, [])


if __name__ == "__main__":
    question = "íŠ¸ëœìŠ¤í¬ë¨¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜"
    #question = "attention is all you needì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜"

    response, _ = hybrid_qa(
        question=question,
        k=3,
        return_sources=False
        )

    print(response)