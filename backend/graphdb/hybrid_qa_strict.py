# hybrid_qa.py

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from vectorstore.vector_qa import run_qa_chain
from graphdb.graph_qa import run_graph_rag_qa  # âœ… fallback ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©


base_dir = os.path.join(os.path.dirname(__file__), "../utils/metadata")
VECTOR_DB_DIR = os.path.join(base_dir, "chroma_db")

llm = ChatOpenAI(model="gpt-4", temperature=0)


# âœ… ë²¡í„° ë¬¸ì„œ title ìš”ì•½ìš© í•¨ìˆ˜
def format_vector_titles(docs: list[Document]) -> str:
    if not docs:
        return "[Vector DB] No documents retrieved."

    result = "[Vector DB] The following document titles were retrieved by similarity search:\n\n"
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get("title", "Unknown Title")
        result += f"{i}. {title}\n"
    return result.strip()


# âœ… Hybrid QA ì‹¤í–‰ í•¨ìˆ˜
def hybrid_qa(
    question: str,
    vector_db_dir=VECTOR_DB_DIR,
    k: int = 3,
    return_sources=False,
    chat_history=[]
):
    print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")

    # âœ… 1. Graph QA ì‹¤í–‰ (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
    graph_answer = run_graph_rag_qa(question, chat_history=chat_history)

    graph_failure_msgs = ["í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§ˆë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”.",
                    "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„ DB ì¡°íšŒë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ (ë²¡í„° DB í˜¹ì€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)ì„ ì´ìš©í•´ì£¼ì„¸ìš”."]

    # âœ… 2. Graph ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•  ê²½ìš° Vector QA ì‹¤í–‰
    if graph_answer in graph_failure_msgs:
        vector_answer, sources = run_qa_chain(
            question, k=k, VECTOR_DB_DIR=vector_db_dir, return_sources=True, chat_history=chat_history
        )
        vector_docs_summary = format_vector_titles(sources)
    else:
        vector_answer, sources = "ê·¸ë˜í”„ DBì—ì„œ ì´ë¯¸ ì¶©ë¶„í•œ ë‚´ìš©ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„°DBë¥¼ ê²€ìƒ‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", []
        vector_docs_summary = ""

    # âœ… 3. System Prompt ì •ì˜
    system_template = SystemMessagePromptTemplate.from_template(
        """You are a helpful assistant. The user may ask questions in any language, and you must respond in the same language.

    You are given two optional answers to assist with your response:

    - Answer A (from Graph DB): {graph_answer}
    - Answer B (from Vector DB): {vector_answer}

    You are also given a list of related document titles retrieved from the Vector DB to help assess the relevance of its content:
    {vector_docs_summary}

    ---

    ğŸ¯ Your task: Respond with a **rich, helpful, and logically sound** answer based primarily on your own academic and domain knowledge.  
    Use the provided answers from the Graph DB and Vector DB to **supplement**, **validate**, or **enhance** your responseâ€”but do **not rely on them exclusively**.

    ---

    ğŸ’¡ Answer generation strategy:

    1. **Start by reasoning independently**:
    - Use your own internal knowledge to understand the user's intent.
    - Formulate a core answer that is helpful, specific, and grounded in academic reasoning.

    2. **Incorporate retrieved DB content where relevant**:
    - Use Graph DB or Vector DB content to reinforce, illustrate, or provide factual backing for your answer.
    - If useful, quote or paraphrase their contentâ€”but **only when it adds value**.

    3. **Evaluate DB content critically**:
    - Ignore responses that are generic, irrelevant, or clearly fallback templates.
    - Do not include misleading or overly vague content from the DB answers.

    ---

    âš ï¸ Important Instructions:
    - Your **primary responsibility is to help the user with your own expertise**.
    - Use the DB results as **supporting tools**, not as mandatory sources.
    - Avoid copying DB answers verbatim unless they contain unique factual insights.
    - Ensure your final answer is clear, complete, and instructive, not just a repetition of retrieved content.

    âœ… At the end of your answer, append exactly one of the following:
    - [Answer Source: Own Knowledge + Graph DB]
    - [Answer Source: Own Knowledge + Vector DB]
    - [Answer Source: Own Knowledge only]
    """
    )



    # âœ… 4. íˆìŠ¤í† ë¦¬ ë°˜ì˜

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_template] + chat_history + [
            HumanMessagePromptTemplate.from_template("{question}")
        ]
    )

    # âœ… 5. LLM ì‹¤í–‰ ì²´ì¸
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({
        "question": question,
        "vector_answer": vector_answer,
        "vector_docs_summary": vector_docs_summary,
        "graph_answer": graph_answer
    })

    # âœ… 6. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if chat_history is not None:
        chat_history.append(HumanMessage(content=question))
        chat_history.append(AIMessage(content=response))

    print("\nğŸ“Œ Hybrid QA Result:")
    print(response)

    return (response, sources) if return_sources else (response, [])


if __name__ == "__main__":
    question = "íŠ¸ëœìŠ¤í¬ë¨¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜"
    #question = "attention is all you needì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜"

    response, _ = hybrid_qa(
        question=question,
        k=3,
        return_sources=False
        )

    print(response)