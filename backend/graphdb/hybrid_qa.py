# hybrid_qa.py

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from vectorstore.qa_chain import run_qa_chain
from graphdb.graph_qa import run_graph_rag_qa  # âœ… fallback ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©


base_dir = os.path.join(os.path.dirname(__file__), "../utils/metadata")
VECTOR_DB_DIR = os.path.join(base_dir, "chroma_db")

llm = ChatOpenAI(model="gpt-4", temperature=0)


# âœ… ë²¡í„° ë¬¸ì„œ title ìš”ì•½ìš© í•¨ìˆ˜
def format_vector_titles(docs: list[Document]) -> str:
    if not docs:
        return "[Vector DB] No documents retrieved."

    result = "[Vector DB] The following document titles were retrieved by similarity search:\n\n"
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get("title", "Unknown Title")
        result += f"{i}. {title}\n"
    return result.strip()


# âœ… Hybrid QA ì‹¤í–‰ í•¨ìˆ˜
def hybrid_qa(
    question: str,
    vector_db_dir=VECTOR_DB_DIR,
    k: int = 3,
    return_sources=False,
    chat_history=[]
):
    print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")

    # âœ… 1. Graph QA ì‹¤í–‰ (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
    graph_answer = run_graph_rag_qa(question)

    # âœ… 2. Graph ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•  ê²½ìš° Vector QA ì‹¤í–‰
    if not graph_answer or graph_answer.strip() == "":
        vector_answer, sources = run_qa_chain(
            question, k=k, VECTOR_DB_DIR=vector_db_dir, return_sources=True
        )
        vector_docs_summary = format_vector_titles(sources)
    else:
        vector_answer, sources = "", []
        vector_docs_summary = ""

    # âœ… 3. System Prompt ì •ì˜
    system_template = SystemMessagePromptTemplate.from_template(
        """You are a helpful assistant. The user may ask in any language, and you must respond in that same language.

    You are given two optional answers to assist you:

    - Answer A (from Vector DB): {vector_answer}
    Related document titles extracted from Vector DB (for you to judge their relevance to the user's question): {vector_docs_summary}

    - Answer B (from Graph DB): {graph_answer}

    Your task is to answer the user's question using the most relevant, informative, and complete source.

    ğŸ” Source selection rules:
    1. First check the **substance and relevance** of the Graph DB answer:
    - If the Graph DB answer contains specific facts that directly and clearly answer the user's question (e.g., citation count, author names, explicit relationships), it can be used.
    - If the answer contains generic fallback text such as:
    - "í˜„ì¬ êµ¬ì¶•ëœ ê·¸ë˜í”„ DBì—ëŠ” ì§ˆë¬¸í•œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    - "ê´€ê³„ê¸°ë°˜ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤."
    then it must be ignored.

    2. If Graph DB is not informative, examine the Vector DB answer and its related document titles:
    - If the answer from Vector DB includes a similar fallback like:
    - "í˜„ì¬ êµ¬ì¶•ëœ ë²¡í„° DBì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    then it must also be ignored.

    - If the document titles are clearly related to the topic of the user's question, and the answer provides helpful information, you may use it.

    3. If neither source is informative or clearly helpful, answer the question using your own general knowledge and reasoning.

    âš ï¸ IMPORTANT:
    - You must **not** select an answer source just because it exists.
    - Prioritize **actual usefulness** of the content, not just presence.
    - Be especially strict when you detect known fallback or template-like phrases in the source answers.

    âœ… Finish your response with exactly one of:
    - [Answer Source: Vector DB]
    - [Answer Source: Graph DB]
    - [Answer Source: Own Knowledge]
    """
    )

    # âœ… 4. íˆìŠ¤í† ë¦¬ ë°˜ì˜
    messages = []
    if chat_history:
        messages.extend(chat_history)

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_template] + messages + [
            HumanMessagePromptTemplate.from_template("{question}")
        ]
    )

    # âœ… 5. LLM ì‹¤í–‰ ì²´ì¸
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({
        "question": question,
        "vector_answer": vector_answer,
        "vector_docs_summary": vector_docs_summary,
        "graph_answer": graph_answer
    })

    # âœ… 6. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if chat_history is not None:
        chat_history.append(HumanMessage(content=question))
        chat_history.append(AIMessage(content=response))

    print("\nğŸ“Œ Hybrid QA Result:")
    print(response)

    return (response, sources) if return_sources else (response, [])


if __name__ == "__main__":
    question = "ì•ˆë…•"

    response, _ = hybrid_qa(
        question=question,
        k=3,
        return_sources=False
        )

    print(response)