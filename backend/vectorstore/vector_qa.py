import os
import sys
from typing import List, Tuple, Union

# âœ… tokenizer warning ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… RefNavi ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ ë¬¸ì œ ë°©ì§€ìš©)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

# âœ… LangChain ìµœì‹  ëª¨ë“ˆ
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ëª…ì‹œì ìœ¼ë¡œ ë¡œë”©
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), ".env")
load_dotenv(dotenv_path)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# âœ… ì„¤ì •
base_dir = os.path.join(os.path.dirname(__file__), "..")
VECTOR_DB_DIR = os.path.join(base_dir, "utils/metadata/chroma_db")
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# âœ… ì „ì—­ embedding + vector DB ì¸ìŠ¤í„´ìŠ¤
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

def run_qa_chain(
    query: str,
    chat_history: List = [],
    k: int = 3,
    VECTOR_DB_DIR=VECTOR_DB_DIR,
    return_sources: bool = False,
) -> Union[str, Tuple[str, List[Document]]]:
    print(f"\nğŸ” ì§ˆì˜: '{query}' â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

    llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=OPENAI_API_KEY)
    db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)

    retrieved_docs = db.similarity_search(query, k=k)
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # âœ… system + history + human message ê¸°ë°˜ prompt êµ¬ì„±
    qa_template = """
You are RefNavi, an academic assistant chatbot that helps users understand scientific papers using retrieved documents and your own knowledge.

Your goal is to answer the user's question as clearly and informatively as possible.

If the retrieved context contains useful information related to the user's question, use it to answer.

If the retrieved documents are **not relevant** or **not helpful**, you MUST say this first:

  "í˜„ì¬ êµ¬ì¶•ëœ ë²¡í„° DBì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹µí•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ìì²´ ì§€ì‹ìœ¼ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤."

Then, proceed to answer using your own general academic knowledge.

Maintain a polite and helpful tone in all responses.

---

ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (Context):
{context}

ì§ˆë¬¸ (Question):
{question}

ë‹µë³€ (Answer):
"""

    system_prompt = SystemMessagePromptTemplate.from_template(qa_template)
    human_prompt = HumanMessagePromptTemplate.from_template("{question}")
    chat_prompt = ChatPromptTemplate.from_messages([
        system_prompt,
        *chat_history,
        human_prompt
    ])

    chain = chat_prompt | llm | StrOutputParser()
    inputs = {"context": context, "question": query}
    answer = chain.invoke(inputs)

    sources = retrieved_docs
    if sources:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
        for i, doc in enumerate(sources, 1):
            print(f"\n--- Source {i} ---")
            print(f"ì œëª©: {doc.metadata.get('title')}")
            print(f"êµ¬ë¶„: {doc.metadata.get('source')}")
            print(f"ì—°ë„: {doc.metadata.get('year')}")
            print(f"ì €ì: {doc.metadata.get('authors')}")
            print(f"ìš”ì•½: {doc.page_content[:300]}...")
    else:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(answer)
    return (answer, sources) if return_sources else (answer, [])

# âœ… ë‹¨ë… ì‹¤í–‰ìš©
if __name__ == "__main__":
    run_qa_chain("ì•ˆë…•")
    run_qa_chain("transformer ë…¼ë¬¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜")
    run_qa_chain("ê·¸ ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ì‹œ ì„¤ëª…í•´ì¤˜")
