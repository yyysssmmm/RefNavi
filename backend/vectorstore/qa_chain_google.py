import os
import sys
from typing import List, Tuple, Union

# âœ… tokenizer warning ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… RefNavi ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ ë¬¸ì œ ë°©ì§€ìš©)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

# âœ… LangChain ìµœì‹  ëª¨ë“ˆ
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# âœ… .env íŒŒì¼ ëª…ì‹œì ìœ¼ë¡œ ë¡œë”©
from dotenv import load_dotenv
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), ".env")
load_dotenv(dotenv_path)

# âœ… API Key ë¡œë”©
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("âŒ GOOGLE_API_KEYê°€ .envì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# âœ… ì„¤ì •
VECTOR_DB_DIR = "chroma_db"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# âœ… ì „ì—­ embedding + vector DB ì¸ìŠ¤í„´ìŠ¤
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

# âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(
    memory_key='chat_history',
    return_messages=True,
    output_key='answer'
)

SYSTEM_PROMPT = """You are a helpful research assistant. 
Answer questions based on the retrieved documents if available. 
If no documents are retrieved, answer using your general knowledge."""

def run_qa_chain(
    query: str,
    k: int = 3,
    VECTOR_DB_DIR: str = "chroma_db",
    return_sources: bool = False,
) -> Union[str, Tuple[str, List[Document]]]:
    print(f"\nğŸ” ì§ˆì˜: '{query}' â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

    # âœ… Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-1.5-flash",
        temperature=0,
        google_api_key=GOOGLE_API_KEY,
        system_prompt = SYSTEM_PROMPT
    )

    db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)

    # âœ… RAG QA ì²´ì¸ ìƒì„±
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=db.as_retriever(search_kwargs={"k": k}),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"output_key": "answer"} 
    )

    result = qa_chain.invoke({"question": query})
    answer = result["answer"]
    sources: List[Document] = result.get("source_documents", [])

    # âœ… ì°¸ì¡° ë¬¸ì„œ ì¶œë ¥
    if sources:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
        for i, doc in enumerate(sources, 1):
            print(f"\n--- Source {i} ---")
            print(f"ì œëª©: {doc.metadata.get('title')}")
            print(f"êµ¬ë¶„: {doc.metadata.get('source')}")
            print(f"ì—°ë„: {doc.metadata.get('year')}")
            print(f"ì €ì: {doc.metadata.get('authors')}")
            print(f"ìš”ì•½: {doc.page_content[:300]}...")
    else:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    return (answer, sources) if return_sources else (answer, [])