import os
import sys
from typing import List, Tuple, Union

# âœ… tokenizer warning ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… RefNavi ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ ë¬¸ì œ ë°©ì§€ìš©)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

# âœ… LangChain ìµœì‹  ëª¨ë“ˆ
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ëª…ì‹œì ìœ¼ë¡œ ë¡œë”©
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), ".env")
load_dotenv(dotenv_path)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# âœ… ì„¤ì •
VECTOR_DB_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../utils/metadata/chroma_db"))
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# âœ… ì „ì—­ embedding + vector DB ì¸ìŠ¤í„´ìŠ¤
try:
    print("ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
except Exception as e:
    print(f"âš ï¸ HuggingFace ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    print("ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë“œë¡œ ì „í™˜...")
    # ë¡œì»¬ ì„ë² ë”© ëª¨ë“œ ì„¤ì •
    os.environ["SENTENCE_TRANSFORMERS_HOME"] = "./models"
    os.environ["TRANSFORMERS_CACHE"] = "./models"
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            cache_folder="./models",
            model_kwargs={'device': 'cpu'}
        )
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ì„ë² ë”© ëª¨ë“œë„ ì‹¤íŒ¨: {str(e)}")
        raise RuntimeError("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# âœ… QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
QA_TEMPLATE = """
You are a research assistant. Based on the following documents, answer the user's query as accurately and concisely as possible.

Context:
{context}

Question: {question}
Answer:
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=QA_TEMPLATE
)

def run_qa_chain(
    query: str,
    k: int = 3,
    return_sources: bool = False,
) -> Union[str, Tuple[str, List[Document]]]:
    print(f"\nğŸ” ì§ˆì˜: '{query}' â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: {VECTOR_DB_DIR}")

    try:
        # ë²¡í„° DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(VECTOR_DB_DIR):
            print(f"âš ï¸ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {VECTOR_DB_DIR}")
            return ("ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", []) if return_sources else "ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

        llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=OPENAI_API_KEY)
        db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)

        # ë²¡í„° DBê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        try:
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
            test_results = db.similarity_search("test", k=1)
            if not test_results:
                print("âš ï¸ ë²¡í„° DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ("ë²¡í„° DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", []) if return_sources else "ë²¡í„° DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        except Exception as e:
            print(f"âš ï¸ ë²¡í„° DB ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")
            return ("ë²¡í„° DB ì ‘ê·¼ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. PDFë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", []) if return_sources else "ë²¡í„° DB ì ‘ê·¼ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. PDFë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=db.as_retriever(search_kwargs={"k": k}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

        result = qa_chain.invoke({"query": query})
        answer = result["result"]
        sources: List[Document] = result["source_documents"]

        if not sources:
            print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.", []) if return_sources else "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        print("\nğŸ“Œ ë‹µë³€:")
        print(answer)
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
        for i, doc in enumerate(sources, 1):
            print(f"\n--- Source {i} ---")
            print(f"ì œëª©: {doc.metadata.get('title')}")
            print(f"êµ¬ë¶„: {doc.metadata.get('source')}")
            print(f"ì—°ë„: {doc.metadata.get('year')}")
            print(f"ì €ì: {doc.metadata.get('authors')}")
            print(f"ìš”ì•½: {doc.page_content[:300]}...")

        return (answer, sources) if return_sources else answer

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise Exception(f"Failed to process query: {str(e)}")

# âœ… ë‹¨ë… ì‹¤í–‰ìš©
if __name__ == "__main__":
    run_qa_chain("What is the contribution of the Transformer paper?", k=5)
