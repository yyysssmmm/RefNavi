import os
import sys
from typing import List, Tuple, Union

# âœ… tokenizer warning ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… RefNavi ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ ë¬¸ì œ ë°©ì§€ìš©)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

# âœ… LangChain ìµœì‹  ëª¨ë“ˆ
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ëª…ì‹œì ìœ¼ë¡œ ë¡œë”©
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), ".env")
load_dotenv(dotenv_path)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# âœ… ì„¤ì •
base_dir = os.path.join(os.path.dirname(__file__), "..")
VECTOR_DB_DIR = os.path.join(base_dir, "utils/metadata/chroma_db")
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# âœ… ì „ì—­ embedding + vector DB ì¸ìŠ¤í„´ìŠ¤
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

memory = ConversationBufferMemory(
    memory_key = 'chat_history',
    return_messages = True,
    output_key='answer'
)

qa_template = """
You are RefNavi, an academic assistant chatbot that helps users understand scientific papers using retrieved documents and your own knowledge.

Your goal is to answer the user's question as clearly and informatively as possible.

If the retrieved context contains useful information related to the user's question, use it to answer.

If the retrieved documents are **not relevant** or **not helpful**, you MUST say this first:

  "í˜„ì¬ êµ¬ì¶•ëœ ë²¡í„° DBì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹µí•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ìì²´ ì§€ì‹ìœ¼ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤."

Then, proceed to answer using your own general academic knowledge.

Maintain a polite and helpful tone in all responses.

---

ì§ˆë¬¸ (Question):
{question}

ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (Context):
{context}

ë‹µë³€ (Answer):
"""

qa_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=qa_template,
)

def run_qa_chain(
    query: str,
    k: int = 3,
    VECTOR_DB_DIR = VECTOR_DB_DIR,
    return_sources: bool = False,

) -> Union[str, Tuple[str, List[Document]]]:
    print(f"\nğŸ” ì§ˆì˜: '{query}' â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

    llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=OPENAI_API_KEY)
    db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=db.as_retriever(search_kwargs={"k": k}),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt":qa_prompt, "output_key": "answer"} 
    )

    result = qa_chain.invoke({"question": query})
    answer = result["answer"]
    sources: List[Document] = result.get("source_documents", [])

    if sources:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
        for i, doc in enumerate(sources, 1):
            print(f"\n--- Source {i} ---")
            print(f"ì œëª©: {doc.metadata.get('title')}")
            print(f"êµ¬ë¶„: {doc.metadata.get('source')}")
            print(f"ì—°ë„: {doc.metadata.get('year')}")
            print(f"ì €ì: {doc.metadata.get('authors')}")
            print(f"ìš”ì•½: {doc.page_content[:300]}...")
    else:
        print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(answer)
    return (answer, sources) if return_sources else (answer, [])

# âœ… ë‹¨ë… ì‹¤í–‰ìš©
if __name__ == "__main__":
    #run_qa_chain("What is the contribution of the Transformer paper?", k=5)
    run_qa_chain("ì•ˆë…•")