import os
import re
import json
import pdfplumber
from typing import Dict, List
from dotenv import load_dotenv
from openai import OpenAI
import nltk
from nltk import sent_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')

# Load API Key
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY missing in .env")
client = OpenAI(api_key=OPENAI_API_KEY)

# ë¬¸ì¥ ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ chunking
def semantic_chunking(text: str, max_chars: int = 6000) -> List[str]:
    sentences = sent_tokenize(text)
    chunks, current_chunk = [], ""

    for sent in sentences:
        if len(current_chunk) + len(sent) + 1 < max_chars:
            current_chunk += " " + sent
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sent
    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_blocks(text: str) -> Dict[str, str]:
    # abstractì™€ referencesì˜ ìœ„ì¹˜ ì°¾ê¸°
    abstract_match = re.search(r'\babstract\b', text, re.IGNORECASE)
    ref_match = re.search(r'\n\s*(references|bibliography)\s*\n', text, re.IGNORECASE)

    # introduction í‚¤ì›Œë“œ ë˜ëŠ” ëª©ì°¨ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ abstract ë ì¶”ì •
    intro_patterns = [
        r'\n\s*(1|â… |I)\.?\s*(introduction)?\s*\n',
        r'\n\s*introduction\s*\n'
    ]
    end_abstract = None
    for pattern in intro_patterns:
        intro_match = re.search(pattern, text[abstract_match.end():], re.IGNORECASE) if abstract_match else None
        if intro_match:
            end_abstract = abstract_match.end() + intro_match.start()
            break
    if not end_abstract:
        end_abstract = abstract_match.end() if abstract_match else 0

    # reference ì‹œì‘ ì§€ì 
    start_refs = ref_match.start() if ref_match else len(text)

    # reference ì´í›„ appendix ë“±ìœ¼ë¡œ ëŠëŠ” êµ¬ê°„
    postfix_patterns = [
        r'\n\s*(appendix|supplementary|acknowledg(e)?ments|about the author|biography)\b'
    ]
    end_refs = len(text)
    for pattern in postfix_patterns:
        match = re.search(pattern, text[start_refs:], re.IGNORECASE)
        if match:
            end_refs = start_refs + match.start()
            break

    # blockë“¤ ì •ì˜
    block1 = text[:end_abstract].strip()
    block2 = text[end_abstract:start_refs].strip()
    block3 = text[start_refs:end_refs].strip()
    block4 = text[end_refs:].strip() if end_refs < len(text) else ""

    return {
        "block1": block1,
        "block2": block2,
        "block3": block3,
        "block4": block4
    }

# LLM í˜¸ì¶œ (1ë‹¨ê³„)
def call_llm_step1(block1: str, block3: str, model="gpt-4"):
    prompt = f"""
[ë…¼ë¬¸ ì •ë³´ ì¼ë¶€]
- ë…¼ë¬¸ ì´ˆë°˜ë¶€ (ì œëª©/ì €ì/abstract í¬í•¨): {block1}
- Reference ì„¹ì…˜ ì „ì²´: {block3}

[ë‹¹ì‹ ì˜ ì„ë¬´]
1. ë…¼ë¬¸ ì œëª©(title)ì„ ê°„ê²°í•˜ê²Œ ì •ì œí•˜ì„¸ìš”.
2. abstract ë‚´ìš©ì€ ìˆ˜ì •í•˜ì§€ ë§ê³ , ë„ì–´ì“°ê¸°ì™€ ë¬¸ì¥ ë¶€í˜¸, ëŒ€ì†Œë¬¸ì, ì˜¤íƒˆìë§Œ êµì •í•˜ì—¬ abstract_originalë¡œ ì¶œë ¥í•˜ì„¸ìš”. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
3. reference sectionì—ì„œ [1], [2], ... í˜•ì‹ì˜ reference ë²ˆí˜¸ë³„ë¡œ ê° ë…¼ë¬¸ì˜ ì œëª©ë§Œ ì¶”ì •í•´ ì¶œë ¥í•˜ì„¸ìš”.

âš ï¸ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…, ì£¼ì„, ì—¬ëŠ” ë§ ì—†ì´ JSONë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "title": "...",
  "abstract_original": "...",
  "references": [
    {{
      "ref_number": "[1]",
      "ref_title": "..."
    }}
  ]
}}
"""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return json.loads(response.choices[0].message.content.strip())

# LLM í˜¸ì¶œ (2ë‹¨ê³„ chunkë³„)
def call_llm_step2_chunk(chunk: str, model="gpt-4") -> Dict:
    prompt = f"""
[ë…¼ë¬¸ ë³¸ë¬¸ ì¼ë¶€ chunk]
{chunk}

[ë‹¹ì‹ ì˜ ì„ë¬´]
1. citation_contextsëŠ” [1], [2], ... í˜•ì‹ì˜ reference ë²ˆí˜¸ê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ì—¬, í•´ë‹¹ ë²ˆí˜¸ë³„ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±í•˜ì„¸ìš”. ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
2. body_fixedëŠ” ë³¸ë¬¸ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, ë„ì–´ì“°ê¸°, êµ¬ë‘ì , ëŒ€ì†Œë¬¸ì, ì˜¤íƒˆìë§Œ êµì •í•˜ì„¸ìš”. ì ˆëŒ€ ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.

âš ï¸ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…, ì£¼ì„, ì—¬ëŠ” ë§ ì—†ì´ JSONë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "body_fixed": "...",
  "citation_contexts": {{
    "[1]": ["..."],
    "[2]": ["..."]
  }}
}}
"""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return json.loads(response.choices[0].message.content.strip())

# ë©”íƒ€ë°ì´í„° ë³‘í•© ë° ì €ì¥
def merge_and_save(step1_result, abstract_llm: str, body_fixed_chunks: List[str], citation_contexts: Dict[str, List[str]], pdf_path: str, out_path: str):
    for ref in step1_result.get("references", []):
        ref_number = ref.get("ref_number")
        ref["citation_contexts"] = citation_contexts.get(ref_number, [])

    final_metadata = {
        "title": step1_result.get("title", ""),
        "abstract_original": step1_result.get("abstract_original", ""),
        "abstract_llm": abstract_llm,
        "body_fixed": "\n\n".join(body_fixed_chunks),
        "references": step1_result.get("references", [])
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(final_metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {out_path}")

# ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def process_pdf(pdf_path: str, out_path: str):
    print(f"\nğŸ“‚ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")

    # âœ… ê¸°ì¡´ ì¶œë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ íŒ¨ìŠ¤
    if os.path.exists(out_path):
        print(f"âš ï¸ ì´ë¯¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬: {out_path} â†’ ì²˜ë¦¬ ìƒëµ")
        return

    with pdfplumber.open(pdf_path) as pdf:
        full_text = "\n\n".join(page.extract_text() for page in pdf.pages if page.extract_text())

    blocks = extract_text_blocks(full_text)

    print("ğŸš€ 1ë‹¨ê³„ LLM í˜¸ì¶œ ì¤‘...")
    step1_result = call_llm_step1(blocks["block1"], blocks["block3"])

    print("ğŸ§  ìš”ì•½ìš© abstract_llm ìƒì„± ì¤‘...")
    abstract_llm_prompt = f"""ë‹¤ìŒì€ ë…¼ë¬¸ ë³¸ë¬¸ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:
{blocks['block2']}"""
    abstract_llm = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": abstract_llm_prompt}],
        temperature=0
    ).choices[0].message.content.strip()

    print("ğŸš€ 2ë‹¨ê³„ LLM ë°˜ë³µ í˜¸ì¶œ ì¤‘...")
    chunks = semantic_chunking(blocks["block2"] + "\n" + blocks["block4"])

    body_fixed_chunks = []
    citation_contexts = {}
    for idx, chunk in enumerate(chunks):
        print(f"  ğŸ” Chunk {idx+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
        result = call_llm_step2_chunk(chunk)
        body_fixed_chunks.append(result.get("body_fixed", ""))
        for ref, ctxs in result.get("citation_contexts", {}).items():
            citation_contexts.setdefault(ref, []).extend(ctxs)

    merge_and_save(step1_result, abstract_llm, body_fixed_chunks, citation_contexts, pdf_path, out_path)

if __name__ == "__main__":
    process_pdf("transformer.pdf", "transformer_metadata.json")
